{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPoAKYJwPeKCwSbZAnYeFbk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"cb4ee9ed"},"source":["#Q1: Explain the differences between AI, ML, Deep Learning (DL), and Data Science (DS).\n","\n","#Answer:\n","\n","Here are the key differences between Artificial Intelligence (AI), Machine Learning (ML), Deep Learning (DL), and Data Science (DS):\n","\n","*   **Artificial Intelligence (AI):** The broadest concept, AI is the simulation of human intelligence processes by machines. It encompasses anything that enables computers to think and behave like humans, including learning, problem-solving, perception, and decision-making. AI is the umbrella term for creating intelligent systems.\n","\n","*   **Machine Learning (ML):** A subset of AI, ML focuses on enabling systems to learn from data without being explicitly programmed. It uses algorithms and statistical models to find patterns in data and make predictions or decisions. ML is a method of achieving AI.\n","\n","*   **Deep Learning (DL):** A subset of ML, DL is inspired by the structure and function of the human brain's neural networks. It uses artificial neural networks with multiple layers (hence \"deep\") to learn complex patterns from large amounts of data. DL is a specific technique within ML that is particularly effective for tasks like image and speech recognition.\n","\n","*   **Data Science (DS):** An interdisciplinary field that uses scientific methods, processes, algorithms, and systems to extract knowledge and insights from structured and unstructured data. Data science encompasses a wide range of activities, including data cleaning, analysis, visualization, and the application of techniques from statistics, computer science, and domain-specific knowledge. While DS often utilizes ML and DL techniques, its scope is broader, focusing on the entire data lifecycle from data collection to communication of findings.\n","\n","In summary:\n","\n","AI is the overall goal (intelligent machines), ML is a way to achieve AI (learning from data), DL is a specific type of ML (using deep neural networks), and Data Science is a field that uses various methods (including ML and DL) to extract insights from data."]},{"cell_type":"markdown","metadata":{"id":"06180bb4"},"source":["#Q2: What are the types of machine learning? Describe each with one\n","real-world example.\n","\n","#Answer:\n","\n","There are three main types of machine learning:\n","\n","*   **Supervised Learning:** This type of learning uses labeled datasets, where both the input data and the desired output are known. The algorithm learns from this data to make predictions on new, unseen data.\n","    *   **Real-world example:** **Spam detection in emails.** A supervised learning model is trained on a dataset of emails that are already labeled as \"spam\" or \"not spam.\" The model learns the patterns and characteristics that differentiate spam emails from legitimate ones and can then predict whether a new email is spam or not.\n","\n","*   **Unsupervised Learning:** This type of learning uses unlabeled datasets. The algorithm's goal is to find hidden patterns, structures, or relationships within the data without any prior knowledge of the desired output.\n","    *   **Real-world example:** **Customer segmentation.** An unsupervised learning algorithm can analyze customer purchase history, demographics, and browsing behavior to group customers into different segments based on their similarities. This can help businesses tailor marketing strategies to specific customer groups.\n","\n","*   **Reinforcement Learning:** This type of learning involves an agent that learns by interacting with an environment. The agent receives rewards for desired actions and penalties for undesired ones, learning through trial and error to maximize its cumulative reward.\n","    *   **Real-world example:** **Training a robot to walk.** A reinforcement learning algorithm can be used to teach a robot to walk. The robot is rewarded for taking steps forward and penalized for falling. Through repeated attempts and learning from the feedback, the robot eventually learns how to walk effectively."]},{"cell_type":"markdown","metadata":{"id":"c2fa1d64"},"source":["#Q3: Define overfitting, underfitting, and the bias-variance tradeoff in machine learning.\n","\n","\n","#Answer:\n","\n","Here are the definitions of overfitting, underfitting, and the bias-variance tradeoff in machine learning:\n","\n","*   **Overfitting:** Overfitting occurs when a machine learning model learns the training data too well, including the noise and outliers. This results in a model that performs very well on the training data but poorly on unseen, new data. An overfitted model is too complex and doesn't generalize well.\n","    *   **Analogy:** Imagine studying for a test by memorizing every single answer to every single practice question, including the typos and incorrect answers. You might ace the practice test, but you'll struggle on the real test if the questions are phrased differently or cover slightly different material.\n","\n","*   **Underfitting:** Underfitting occurs when a machine learning model is too simple to capture the underlying patterns in the training data. This results in a model that performs poorly on both the training data and unseen data. An underfitted model is not complex enough to learn the relationships in the data.\n","    *   **Analogy:** Imagine trying to understand a complex topic by only reading the first paragraph of an introductory text. You won't have enough information to grasp the nuances and will likely perform poorly on any test related to the topic.\n","\n","*   **Bias-Variance Tradeoff:** The bias-variance tradeoff is a fundamental concept in machine learning that describes the relationship between a model's ability to generalize to new data and its complexity.\n","    *   **Bias:** Bias is the error introduced by approximating a real-world problem, which may be complex, by a simplified model. High bias means the model is too simple and underfits the data.\n","    *   **Variance:** Variance is the error introduced by the model's sensitivity to small fluctuations in the training data. High variance means the model is too complex and overfits the data.\n","    *   **Tradeoff:** The tradeoff is that reducing bias often increases variance, and reducing variance often increases bias. The goal is to find a balance between bias and variance to build a model that generalizes well to unseen data. A good model will have low bias and low variance, but achieving this perfectly is often impossible. The optimal model complexity lies somewhere in the middle, minimizing the total error (bias squared + variance + irreducible error)."]},{"cell_type":"markdown","metadata":{"id":"499166d4"},"source":["#Q4: What are outliers in a dataset, and list three common techniques for handling them.\n","\n","\n","#Answer:\n","\n","**Outliers** are data points in a dataset that are significantly different from other observations. They can occur due to measurement errors, data entry errors, or they might represent genuine but extreme variations in the data. Outliers can negatively impact the results of data analysis and machine learning models.\n","\n","Here are three common techniques for handling outliers:\n","\n","1.  **Removal (Deletion):** This is the simplest method, where the outlier data points are removed from the dataset. This technique should be used cautiously, especially if the dataset is small, as it can lead to loss of valuable information and may not be appropriate if the outliers represent genuine extreme values.\n","2.  **Transformation:** This involves applying a mathematical transformation to the data to reduce the impact of outliers. Common transformations include logarithmic, square root, or reciprocal transformations. This can help to normalize the data distribution and make outliers less influential.\n","3.  **Imputation:** This involves replacing the outlier values with a more representative value. This could be the mean, median, or mode of the data, or a value predicted by another model. Imputation helps to retain the data points and avoid information loss, but the choice of imputation method can significantly affect the results. Other methods include using the interquartile range (IQR) to define boundaries and cap or floor the outliers within those boundaries."]},{"cell_type":"markdown","metadata":{"id":"1d516435"},"source":["#Q5: Explain the process of handling missing values and mention one imputation technique for numerical and one for categorical data.\n","\n","#Answer:\n","\n","Handling missing values is a crucial step in data preprocessing, as missing data can lead to biased results and reduced model performance. The process typically involves the following steps:\n","\n","1.  **Identify missing values:** The first step is to identify which columns contain missing values and how many. This can be done visually or programmatically.\n","2.  **Understand the reason for missingness:** It's important to understand why the data is missing. Is it random, or is there a pattern? The reason for missingness can influence the choice of handling technique.\n","3.  **Choose a handling technique:** Based on the amount of missing data, the type of data, and the reason for missingness, choose an appropriate technique. Common techniques include:\n","    *   **Deletion:** Remove rows or columns with missing values. This is suitable when there are very few missing values or if a whole column is missing a large percentage of data.\n","    *   **Imputation:** Replace missing values with estimated values.\n","    *   **Ignoring missing values:** Some machine learning algorithms can handle missing values internally.\n","\n","Here is one imputation technique for numerical and one for categorical data:\n","\n","*   **Numerical Data Imputation (Mean/Median Imputation):** A common technique for numerical data is to replace missing values with the mean or median of the non-missing values in that column. The median is often preferred when the data has outliers, as it is less sensitive to extreme values than the mean.\n","    *   **Example:** If a column representing 'Age' has missing values, you could calculate the median age of the available data and fill the missing entries with that median value.\n","\n","*   **Categorical Data Imputation (Mode Imputation):** For categorical data, a common technique is to replace missing values with the mode (the most frequent value) of the non-missing values in that column.\n","    *   **Example:** If a column representing 'City' has missing values, you could find the most frequent city among the available data and fill the missing entries with that city name."]},{"cell_type":"markdown","source":["#Q6: Write a Python program that:\n","● Creates a synthetic imbalanced dataset with make_classification() from\n","sklearn.datasets.\n","● Prints the class distribution.\n","(Include your Python code and output in the code box below.)\n","\n"," #Answer:\n"],"metadata":{"id":"SLvf-yHGBDcq"}},{"cell_type":"code","metadata":{"id":"5e13ca94"},"source":["from sklearn.datasets import make_classification\n","import pandas as pd\n","\n","# Create a synthetic imbalanced dataset\n","X, y = make_classification(n_samples=1000, n_features=20, n_informative=2,\n","                           n_redundant=10, n_classes=2, weights=[0.9, 0.1],\n","                           flip_y=0, random_state=1)\n","\n","# Convert to a pandas Series for easier class distribution analysis\n","y_series = pd.Series(y)\n","\n","# Print the class distribution\n","print(\"Class distribution:\")\n","print(y_series.value_counts())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Q7  Implement one-hot encoding using pandas for the following list of colors:\n","['Red', 'Green', 'Blue', 'Green', 'Red']. Print the resulting dataframe.\n","(Include your Python code and output in the code box below.)\n","\n"," #Answer:\n","  \n","  \n"],"metadata":{"id":"78wNCLV-BVbB"}},{"cell_type":"code","metadata":{"id":"8b4a7aa7"},"source":["import pandas as pd\n","\n","colors = ['Red', 'Green', 'Blue', 'Green', 'Red']\n","\n","# Convert the list to a pandas Series\n","colors_series = pd.Series(colors)\n","\n","# Apply one-hot encoding\n","one_hot_encoded = pd.get_dummies(colors_series)\n","\n","# Print the resulting dataframe\n","print(\"One-hot encoded dataframe:\")\n","print(one_hot_encoded)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Q8  Write a Python script to:\n","● Generate 1000 samples from a normal distribution.\n","● Introduce 50 random missing values.\n","● Fill missing values with the column mean.\n","● Plot a histogram before and after imputation.\n","(Include your Python code and output in the code box below.)\n","\n"," #Answer:\n","\n","  \n"],"metadata":{"id":"M1n7Jy5LB1Um"}},{"cell_type":"code","metadata":{"id":"98e15c9a"},"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","# Generate 1000 samples from a normal distribution\n","np.random.seed(42) # for reproducibility\n","data = np.random.normal(loc=0, scale=1, size=1000)\n","df = pd.DataFrame(data, columns=['Value'])\n","\n","# Introduce 50 random missing values\n","missing_indices = np.random.choice(df.index, size=50, replace=False)\n","df.loc[missing_indices, 'Value'] = np.nan\n","\n","# Plot histogram before imputation\n","plt.figure(figsize=(10, 5))\n","plt.subplot(1, 2, 1)\n","plt.hist(df['Value'].dropna(), bins=30, edgecolor='black')\n","plt.title('Histogram Before Imputation')\n","plt.xlabel('Value')\n","plt.ylabel('Frequency')\n","\n","# Fill missing values with the column mean\n","df['Value_Imputed'] = df['Value'].fillna(df['Value'].mean())\n","\n","# Plot histogram after imputation\n","plt.subplot(1, 2, 2)\n","plt.hist(df['Value_Imputed'], bins=30, edgecolor='black')\n","plt.title('Histogram After Mean Imputation')\n","plt.xlabel('Value')\n","plt.ylabel('Frequency')\n","\n","plt.tight_layout()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Q9  Implement Min-Max scaling on the following list of numbers [2, 5, 10, 15,\n","20] using sklearn.preprocessing.MinMaxScaler. Print the scaled array.\n","(Include your Python code and output in the code box below.)\n","\n"," #Answer:\n","\n","  \n"],"metadata":{"id":"NeEL-gL0CRva"}},{"cell_type":"code","metadata":{"id":"1475405c"},"source":["from sklearn.preprocessing import MinMaxScaler\n","import numpy as np\n","\n","# List of numbers\n","data = np.array([2, 5, 10, 15, 20]).reshape(-1, 1)\n","\n","# Initialize the MinMaxScaler\n","scaler = MinMaxScaler()\n","\n","# Apply Min-Max scaling\n","scaled_data = scaler.fit_transform(data)\n","\n","# Print the scaled array\n","print(\"Scaled array:\")\n","print(scaled_data)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cfc1fae1"},"source":["#Q10  You are working as a data scientist for a retail company. You receive a customer\n","transaction dataset that contains:\n","● Missing ages,\n","● Outliers in transaction amount,\n","● A highly imbalanced target (fraud vs. non-fraud),\n","● Categorical variables like payment method.\n","Explain the step-by-step data preparation plan you’d follow before training a machine learning\n","model. Include how you’d address missing data, outliers, imbalance, and encoding.\n","(Include your Python code and output in the code box below.)\n","\n","#Answer:\n","\n","Here is a step-by-step data preparation plan to address the issues in the customer transaction dataset before training a machine learning model, with code examples for key steps:\n","\n","1.  **Load the dataset:** Load the customer transaction data into a pandas DataFrame."]},{"cell_type":"code","metadata":{"id":"3525a61d"},"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from imblearn.over_sampling import SMOTE\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","\n","# Create a synthetic dataset for demonstration\n","data = {\n","    'Age': np.random.randint(18, 70, 1000).astype(float),\n","    'Transaction_Amount': np.random.normal(50, 30, 1000),\n","    'Payment_Method': np.random.choice(['Credit Card', 'Debit Card', 'E-wallet', 'Bank Transfer'], 1000),\n","    'Is_Fraud': np.random.choice([0, 1], 1000, p=[0.95, 0.05]) # Highly imbalanced\n","}\n","df = pd.DataFrame(data)\n","\n","# Introduce some missing values in 'Age'\n","missing_indices_age = np.random.choice(df.index, size=50, replace=False)\n","df.loc[missing_indices_age, 'Age'] = np.nan\n","\n","# Introduce some outliers in 'Transaction_Amount'\n","outlier_indices = np.random.choice(df.index, size=10, replace=False)\n","df.loc[outlier_indices, 'Transaction_Amount'] = np.random.uniform(200, 500, 10)\n","\n","print(\"Initial DataFrame head:\")\n","print(df.head())\n","print(\"\\nInitial DataFrame info:\")\n","print(df.info())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"adc07073"},"source":["2.  **Handle Missing Values (Missing Ages):**\n","    *   **Identify missing values:** Check the 'Age' column for missing values.\n","    *   **Choose an imputation strategy:** Impute missing 'Age' values with the median.\n","    *   **Implement the chosen strategy:** Apply median imputation."]},{"cell_type":"code","metadata":{"id":"fc779366"},"source":["print(\"\\nMissing values before imputation:\")\n","print(df.isnull().sum())\n","\n","# Impute missing 'Age' values with the median\n","median_age = df['Age'].median()\n","df['Age'].fillna(median_age, inplace=True)\n","\n","print(\"\\nMissing values after Age imputation:\")\n","print(df.isnull().sum())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"919cb333"},"source":["3.  **Handle Outliers (Transaction Amount):**\n","    *   **Identify outliers:** Visualize the distribution and use IQR to identify potential outliers.\n","    *   **Choose an outlier handling technique:** Cap and floor outliers based on IQR.\n","    *   **Implement the chosen technique:** Apply capping and flooring."]},{"cell_type":"code","metadata":{"id":"841568bf"},"source":["print(\"\\nDistribution of Transaction Amount before outlier handling:\")\n","plt.figure(figsize=(8, 4))\n","sns.boxplot(x=df['Transaction_Amount'])\n","plt.title('Box Plot of Transaction Amount Before Outlier Handling')\n","plt.show()\n","\n","Q1 = df['Transaction_Amount'].quantile(0.25)\n","Q3 = df['Transaction_Amount'].quantile(0.75)\n","IQR = Q3 - Q1\n","lower_bound = Q1 - 1.5 * IQR\n","upper_bound = Q3 + 1.5 * IQR\n","\n","print(f\"Q1: {Q1}, Q3: {Q3}, IQR: {IQR}\")\n","print(f\"Lower bound for outliers: {lower_bound}\")\n","print(f\"Upper bound for outliers: {upper_bound}\")\n","\n","outliers = df[(df['Transaction_Amount'] < lower_bound) | (df['Transaction_Amount'] > upper_bound)]\n","print(f\"\\nNumber of outliers identified: {len(outliers)}\")\n","\n","# Cap and floor outliers in 'Transaction_Amount'\n","df['Transaction_Amount_Capped'] = df['Transaction_Amount'].clip(lower=lower_bound, upper=upper_bound)\n","\n","print(\"\\nDistribution of Transaction Amount after outlier handling (Capping):\")\n","plt.figure(figsize=(8, 4))\n","sns.boxplot(x=df['Transaction_Amount_Capped'])\n","plt.title('Box Plot of Transaction Amount After Outlier Handling (Capping)')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d1233bd6"},"source":["4.  **Encode Categorical Variables (Payment Method):**\n","    *   **Identify categorical variables:** Identify the 'Payment Method' column.\n","    *   **Choose an encoding technique:** Use One-Hot Encoding for 'Payment_Method'.\n","    *   **Implement the chosen technique:** Apply One-Hot Encoding."]},{"cell_type":"code","metadata":{"id":"0a83fb1d"},"source":["# Apply One-Hot Encoding to 'Payment_Method'\n","df = pd.get_dummies(df, columns=['Payment_Method'], drop_first=True)\n","\n","print(\"\\nDataFrame after One-Hot Encoding:\")\n","print(df.head())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"668a3d1e"},"source":["5.  **Address Class Imbalance (Fraud vs. Non-Fraud):**\n","    *   **Analyze class distribution:** Determine the ratio of fraud to non-fraud transactions.\n","    *   **Choose an imbalance handling technique:** Use SMOTE to oversample the minority class (Fraud).\n","    *   **Implement the chosen technique:** Apply SMOTE to the training data."]},{"cell_type":"code","metadata":{"id":"051791f0"},"source":["print(\"\\nClass distribution of 'Is_Fraud' before handling imbalance:\")\n","print(df['Is_Fraud'].value_counts())\n","print(df['Is_Fraud'].value_counts(normalize=True) * 100)\n","\n","# Separate features and target\n","X = df.drop(['Is_Fraud', 'Transaction_Amount'], axis=1) # Drop original amount and target\n","y = df['Is_Fraud']\n","\n","# Split into training and testing sets (important to apply SMOTE only on training data)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n","\n","# Apply SMOTE to the training data\n","smote = SMOTE(random_state=42)\n","X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n","\n","print(\"\\nClass distribution of 'Is_Fraud' after SMOTE (on training data):\")\n","print(pd.Series(y_train_resampled).value_counts())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b459d0b1"},"source":["6.  **Feature Scaling (Optional but recommended):**\n","    *   Apply feature scaling to numerical features if using algorithms sensitive to scale.\n","    *   Use Standardization (Z-score scaling)."]},{"cell_type":"code","metadata":{"id":"1c9545bf"},"source":["# Identify numerical columns for scaling (excluding the original 'Transaction_Amount' if capped version is used)\n","numerical_cols = ['Age', 'Transaction_Amount_Capped'] # Use the capped version\n","\n","# Initialize the scaler\n","scaler = StandardScaler()\n","\n","# Fit and transform the numerical columns in the resampled training data\n","X_train_resampled[numerical_cols] = scaler.fit_transform(X_train_resampled[numerical_cols])\n","\n","# Transform the numerical columns in the test data using the scaler fitted on training data\n","X_test[numerical_cols] = scaler.transform(X_test[numerical_cols])\n","\n","print(\"\\nTraining data after Feature Scaling:\")\n","print(X_train_resampled.head())\n","print(\"\\nTest data after Feature Scaling:\")\n","print(X_test.head())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a569758b"},"source":["7.  **Split the data:** The data has already been split into training and testing sets in the imbalance handling step.\n","\n","8.  **Model Training:** Train the chosen machine learning model on the prepared training data (`X_train_resampled`, `y_train_resampled`).\n","\n","9.  **Model Evaluation:** Evaluate the model's performance on the testing set (`X_test`, `y_test`) using appropriate metrics for imbalanced datasets (precision, recall, F1-score, AUC-ROC)."]}]}